{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f449a25c",
   "metadata": {},
   "source": [
    "# Presentation of classification algorithm Matthias Walter 1780559, october 2025\n",
    "\n",
    "\n",
    "## **Data cleaning**\n",
    "Start with data cleaning. Goal: use loop through all folders to get RGB pixel data of 64*64 images and append their object ID. \n",
    "Create 1D vectors of pixels and their respective objectID, (starts with 123 ) and match with Zoospec datasets provided on CV. From Zoospec table we use the object ID for matching the pixel data with their labels \"spiral\" or \"elliptical\". \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89a1380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "build = False\n",
    "## data cleaning start --------------------------------------------------------------------------------------------------------------------------------\n",
    "# read data for information on classification information as a function ob object id \n",
    "#drop unecessary columns for better overview \n",
    "if build:\n",
    "    classification_df = pd.read_csv(\"ZooSpecPhotoDR19_torradeflot.csv\", usecols=['objid', 'spiral', 'elliptical'])\n",
    "\n",
    "\n",
    "\n",
    "    directory = r\"C:\\Users\\matth\\OneDrive - UniversitÃ¤t Graz\\Uni\\Master\\Semester4\\Statistics and Data analysis\\project_galaxy_data\\HIPS2FITS_petro\"\n",
    "\n",
    "\n",
    "    all_image_data = []\n",
    "\n",
    "\n",
    "    #apparently os.walk goes through all subfolders and files in a dir\n",
    "    # .ipynb checkpoints folder and 0.jpg was just deleted for simplicity\n",
    "    for root,_, files in os.walk(directory):\n",
    "        print(f\"Processing folder: {root}\") # show current folder being processed because of large folder number\n",
    "        # 'root' is the path to the current subfolder for, like 99\n",
    "        # 'files' is a list of all filenames in that subfolder, with their object id asd name \n",
    "\n",
    "        for file_name in files:\n",
    "            # Process only images and avoid iterating the folder in the single subfolders \n",
    "            if file_name.lower().endswith(('.jpg', '.jpeg')): # \n",
    "                # split datatype specific ending from filenamem in this case .jpg\n",
    "                # because filename is objectid and  this is needed for later matching\n",
    "                objid = os.path.splitext(file_name)[0]\n",
    "                # abs path\n",
    "                image_path = os.path.join(root, file_name)\n",
    "                \n",
    "                # create vector of that image\n",
    "                with Image.open(image_path) as img:\n",
    "                    # attention: images are in RGB, so there is 3 dimensions of 64x64 pixels (previous mistake)\n",
    "\n",
    "                    # Ensure image is in RGB mode (3 channels)\n",
    "                    img_rgb = img.convert('RGB')\n",
    "                    \n",
    "                    # Convert the 64x64x3 image to a NumPy array\n",
    "                    pixel_array = np.array(img_rgb)\n",
    "                    \n",
    "                    # Separate the channels and flatten each one\n",
    "                    r_channel = pixel_array[:, :, 0].flatten() # All red pixels\n",
    "                    g_channel = pixel_array[:, :, 1].flatten() # All green pixels\n",
    "                    b_channel = pixel_array[:, :, 2].flatten() # All blue pixels\n",
    "                    \n",
    "                    # 2D-> 1D: Concatenate all channels into one long row\n",
    "                    flattened_pixels = np.concatenate([r_channel, g_channel, b_channel])\n",
    "                    \n",
    "                    # append objectid name                  \n",
    "                    row = [objid] + flattened_pixels.tolist()\n",
    "\n",
    "                    # Add this row to our main data list\n",
    "                    all_image_data.append(row)\n",
    "\n",
    "    print(\"Image processing complete.\")\n",
    "\n",
    "    # convert to pandas df \n",
    "\n",
    "    # create vectors of pixels ordered, first all red pixels then all green then all blue pixels\n",
    "    num_pixels = 64 * 64  # 4096 pixels per RGB dimension\n",
    "\n",
    "    # Create names for each channel, use i+1 because of python indexing starting at 0 \n",
    "\n",
    "\n",
    "    # this is not necessary it just produceds a nicer data frame \n",
    "    r_cols = [f'R_pix_{i+1}' for i in range(num_pixels)]\n",
    "    g_cols = [f'G_pix_{i+1}' for i in range(num_pixels)]\n",
    "    b_cols = [f'B_pix_{i+1}' for i in range(num_pixels)]\n",
    "\n",
    "\n",
    "\n",
    "    # Combine all column names\n",
    "    column_names = ['objid'] + r_cols + g_cols + b_cols\n",
    "\n",
    "\n",
    "    # Create the DataFrame\n",
    "    image_df = pd.DataFrame(all_image_data, columns=column_names)\n",
    "\n",
    "    # make sure datatypes match (previous mistake because str and int cannot match of course)\n",
    "    image_df['objid'] = image_df['objid'].astype(str)\n",
    "    classification_df['objid'] = classification_df['objid'].astype(str)\n",
    "\n",
    "    # now use objid to merge both dataframes\n",
    "    final_df = pd.merge(image_df, classification_df, on='objid')\n",
    "\n",
    "\n",
    "    # data is not always spiral or elliptical , some have 0 in both columns, those will be dropped\n",
    "    print(f\"Count of (0, 0) cases: {len(final_df[(final_df['spiral'] == 0) & (final_df['elliptical'] == 0)])}\")\n",
    "    clean_index = (final_df['spiral'] == 1) | (final_df['elliptical'] == 1)\n",
    "\n",
    "    # final df for classification task \n",
    "    final_df = final_df[clean_index]\n",
    "\n",
    "\n",
    "    # Display the first 5 rows for checking\n",
    "    print(\"Merged DataFrame preview:\")\n",
    "    print(final_df.head())\n",
    "\n",
    "    #save to csv for resuse (iteration through all of the images takes long if algorithm has to be adapted)\n",
    "    final_df.to_csv('new_galaxy_data.csv', index=False)\n",
    "\n",
    "    # if build is false just read the csv that has previously been built by the exact algorithm above\n",
    "else:\n",
    "    final_df = pd.read_csv('new_galaxy_data.csv')\n",
    "\n",
    "## data cleaning end --------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc38b3e6",
   "metadata": {},
   "source": [
    "### Data cleaning- what to look out for\n",
    "* RGB data, 3 dimensions of 64^2 pixels \n",
    "* ipynb folder inside image folder 00 \n",
    "* matching of different data types (str <-> int) when comparing objid from different dataframes\n",
    "* Some objects were neither labeled \"spiral\" nor \"elliptical\", those entries were omitted, as they are not useful for the binary output of the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6dd3a9",
   "metadata": {},
   "source": [
    "## **Training the ML classification algorithm**\n",
    "\n",
    "### Data preparation\n",
    "The cleaned data is now prepared for a machine learning model to be easily \"digestable\". \n",
    "The goal is to use the pixel images as input for the machine learning algorithm to perform binary classification on \"spiral\" \"elliptical\". \n",
    "We prepared the data by setting the labels either \"spiral\" or \"elliptical\", which means one label vector with values 0,1 can be used for as a target variable, as a 1 in spiral implies 0 in elliptical and vice versa. \n",
    "\n",
    "For PCA the components of most variance are kept. Why? Areas of the domain that are constant always have less information than areas that are dependend on the domain, as we want to learn how a function behaves in differnet regions of the domain. In context this means that pixels (=features) that are always dark don't have a lot of variance across different images and therefore give less information than pixels that are always changing, by looking at pixels we look at discrete domains of a function, **the difference between two pictures could therefore be distiguished identically if the always dark pixel would have been neglected, but less data would have to be stored -> faster model.**\n",
    "\n",
    "Although what matters for distinction of images is the variance of a discrete region of the function (an image), it is not the absolute variance that matters, as e.g. some region of the center region of the images are always brighter than the edges, but the relative variance. Therefore all of the features (= pixels) will be rescaled by StandardRescaler, which means that all values of each pixel will be rescaled, such that they have $\\mu$= 0 and $\\sigma = 1$. \n",
    "\n",
    "\n",
    "Then the principal components of the vector are taken, but not so fast as the data set is quite big and my PC quite slow, that is why incremental PCA is used. \n",
    "This looks at the data consecutively and retrieves a given number of components. The number of components is first estimated by taking a smaller batch of data, this is a bit of a trade of but computationally necessary.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c403585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features: 12288\n",
      "PCA-reduced features: 577\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "## now to the fun part -------------------------------------------------------------------------------------------------------------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "\n",
    "y = final_df['spiral']  # define spiral label as target variable, 0 implies 1 in elliptical see data cleaning\n",
    "X = final_df.drop(columns=['objid', 'spiral', 'elliptical'])  # we only selcect pixel values for machine learning model to learn from \n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.3,    # 30% of data for testing \n",
    "    random_state=69,  # for reproducibility \n",
    "    stratify=y        # this makes sure that train and test data have the same proportion of classes -> better training quality\n",
    ")\n",
    "\n",
    "# The \n",
    "# A bright pixel (value 250) would be treated as \"more important\"\n",
    "# than a dim one (value 20).\n",
    "# StandardScaler rescales all  features to have a mean of 0\n",
    "# and a standard deviation of 1.\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#  fit the scaler ONLY on the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# transform the test data using the same scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "#because there were some problems with my ram i now do incremental pca, this requires a batch size as input \n",
    "\n",
    "n_sample_size = 5000 \n",
    "pca_finder = PCA(n_components=0.95, random_state= 69)\n",
    "pca_finder.fit(X_train_scaled[:n_sample_size]) # Fit on just 5000 rows\n",
    "\n",
    "# Get the number of components it found\n",
    "k = pca_finder.n_components_\n",
    "\n",
    "\n",
    "pca = IncrementalPCA(n_components=k, batch_size = 1000)\n",
    "\n",
    "\n",
    "# Fit PCA ONLY on the training data\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "# Transform the test data using the same PCA fit\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "print(f\"Original features: {X_train_scaled.shape[1]}\")\n",
    "print(f\"PCA-reduced features: {X_train_pca.shape[1]}\")\n",
    "print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28498ecf",
   "metadata": {},
   "source": [
    "## **Choice, training and evaluation of a ML model**\n",
    "### Training\n",
    "The test data is now used for training a ML algorithm (I use algorithms from sklearn library), $X_{train}$ and $y_{train}$ are used for training and the trained model is then used to create a prediction based on the until now omitted test data $X_{test}$. The results of the predictions $y_{pred}$ are then compared to the actual labels of the features $y_{test}$. \n",
    "\n",
    "### Evaluation\n",
    "The meaning of the output of the classification report is the following\n",
    "* **precision**: how many of the classified as \"spiral\" galaxies, were actually \"spiral\"?, how easily do I get false positives\n",
    "* **recall**: how many of the \"spiral\" galaxies were identified as such? how easily do I get false negatives\n",
    "* **f1 score**: prec*recall/(prec+recall)*2 \n",
    "* **accuracy**: how many times was the prediction right? dangerous because not 50/50 occurance of both data types \n",
    "* **macro average**: accuracy weighted by f1 score for each binary output, considers above problem\n",
    "\n",
    "\n",
    "### Used model\n",
    "#### Random tree\n",
    "A random tree is constructed like a flowchart that asks questions to the input data and eventually gives an output, a clasification. A question in this case could be: is the red value of pixel 138 > 0.7 -> spiral, then other questions would be asked consequtively to get to a final result. The actual learning takes place in the way the content, order and  parameters of those questions are chosen. This is done as they are chosen arbitrarily with the goal of entropy minimization of the data. This means that one question should be chosen such, that it separates \"spiral\" from \"elliptical\" as effectively as possible. Then additional questions are asked until domains of only \"spiral\" and \"elliptical\" data points are grouped by the decision tree. In the used algorithm the Shannon-entropy is used $$H(S) = - \\sum_{i=1}^{C} p_i \\log_2(p_i)$$ with C being the number of classes, in our case 2 and $p_i$ being the fraction of elements in a node S that belong to a certain class. \n",
    "\n",
    "#### Random forest\n",
    "A random forest consist of many random trees, however not every tree in the forest is trained on the whole data. Every tree is trained on bootstrapped subbatches of the original data and gives a different output for example tree 1 votes \"spiral\" and tree 3 votes \"elliptical\" all different because trained on different domains of the dataset. Then an average can be taken for the final result, which prevents overfitting and overinterpretation of noise in a single decision trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5af72a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.74      0.84      3825\n",
      "           1       0.94      1.00      0.97     16052\n",
      "\n",
      "    accuracy                           0.95     19877\n",
      "   macro avg       0.96      0.87      0.90     19877\n",
      "weighted avg       0.95      0.95      0.94     19877\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 2816  1009]\n",
      " [   49 16003]]\n"
     ]
    }
   ],
   "source": [
    "# actual training now \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "# Initialize the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=69)\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_classifier.fit(X_train_pca, y_train)\n",
    "# Make predictions on the test data\n",
    "y_pred = rf_classifier.predict(X_test_pca)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred)) \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
